[Wed, 03 Nov 2021 03:21:16] INFO [main: run_main_batch.py, 64] pid:387, epoch:50
[Wed, 03 Nov 2021 03:21:16] INFO [main: run_main_batch.py, 65] args:Namespace(Fold=1, add_analogy_embedding=True, add_copynet=True, add_memory_module=True, add_num_equ_ids=True, add_tokens_a_padding=True, always_truncate_tail=True, amp=False, attention_probs_dropout_prob=0.1, batch_size=20, beam_size=1, bert_model='bert-base-chinese', config_path=None, copynet_name='copynet2', dataset='math23k_test1k', do_eval=False, do_l2r_training=False, do_lower_case=False, do_train=True, easy_to_hard=True, eval_batch_size=64, ffn_type=0, finetune_decay=False, forbid_duplicate_ngrams=False, forbid_ignore_word='.', fp16=False, fp32_embedding=False, from_scratch=False, gradient_accumulation_steps=1, has_sentence_oracle=False, hidden_dropout_prob=0.1, ids_questions_path='./preprocess/raw_sim_dict/raw_math23k_test1k_data_dict.pkl', is_debug=False, is_delete_early_model=False, is_equ_norm=True, is_single_char=False, is_train=True, label_smoothing=0.1, learning_rate=2e-05, length_penalty=0, local_rank=-1, log_dir='./comment/math23k/model_log', loss_scale=0, mask_prob=0.15, mask_prob_eos=0, mask_source_words=False, mask_whole_word=True, max_analogy_len=512, max_len_a=192, max_len_b=64, max_position_embeddings=512, max_pred=64, max_seq_length=256, max_tgt_length=64, memory_train_file='./preprocess/sim_result/sim_question_by_w2v_train_math23k_test1000_equNorm_top10.pkl', memory_valid_file='./preprocess/sim_result/sim_question_by_w2v_valid_math23k_test1k_equNorm_top10.pkl', min_len=None, mode='s2s', model_recover_path='./comment/math23k/REAL2_math23k_test1k_topn10_top3_cnn/model.epoch.bin', need_score_traces=True, new_pos_ids=False, new_segment_ids=False, ngram_size=3, no_cuda=False, not_predict_token=None, num_equ_size=3, num_qkv=0, num_train_epochs=50, num_workers=0, optim_recover_path=None, output_dir='./comment/math23k/REAL2_math23k_test1k_topn10_top3_cnn', pos_shift=True, pred_wo_memory_copy=False, relax_projection=False, repeat='', retrieve_model_name='cnn', retrieve_result_path='', retrieve_topn=10, s2s_add_segment=False, s2s_share_segment=False, s2s_special_token=False, save_every_epoch=False, seed=42, seg_emb=False, skipgram_prb=0.0, skipgram_size=1, split='test', start_lr_decay_epoch=25, subset=0, tokenized_input=False, topk=3, train_batch_size=1, train_question_sim_ids_path='./preprocess/raw_sim_dict/math23k_test1k_train_sim_id_top10.pkl', trunc_seg='a', used_bertAdam=False, used_lr_decay=True, valid_question_sim_ids_path='./preprocess/raw_sim_dict/math23k_test1k_valid_sim_id_top10.pkl', warmup_proportion=0.1, weight_decay=0.01)
[Wed, 03 Nov 2021 03:21:17] INFO [main: run_seq2seq_mwp.py, 71] device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
[Wed, 03 Nov 2021 03:21:18] INFO [from_pretrained: tokenization.py, 197] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at ..comment/tmp_folder/bert-cased-pretrained-cache/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00
[Wed, 03 Nov 2021 03:22:05] INFO [main: run_main_batch.py, 64] pid:474, epoch:50
[Wed, 03 Nov 2021 03:22:05] INFO [main: run_main_batch.py, 65] args:Namespace(Fold=1, add_analogy_embedding=True, add_copynet=True, add_memory_module=True, add_num_equ_ids=True, add_tokens_a_padding=True, always_truncate_tail=True, amp=False, attention_probs_dropout_prob=0.1, batch_size=20, beam_size=1, bert_model='bert-base-chinese', config_path=None, copynet_name='copynet2', dataset='math23k_test1k', do_eval=False, do_l2r_training=False, do_lower_case=False, do_train=True, easy_to_hard=True, eval_batch_size=64, ffn_type=0, finetune_decay=False, forbid_duplicate_ngrams=False, forbid_ignore_word='.', fp16=False, fp32_embedding=False, from_scratch=False, gradient_accumulation_steps=1, has_sentence_oracle=False, hidden_dropout_prob=0.1, ids_questions_path='./preprocess/raw_sim_dict/raw_math23k_test1k_data_dict.pkl', is_debug=False, is_delete_early_model=False, is_equ_norm=True, is_single_char=False, is_train=True, label_smoothing=0.1, learning_rate=2e-05, length_penalty=0, local_rank=-1, log_dir='./comment/math23k/model_log', loss_scale=0, mask_prob=0.15, mask_prob_eos=0, mask_source_words=False, mask_whole_word=True, max_analogy_len=512, max_len_a=192, max_len_b=64, max_position_embeddings=512, max_pred=64, max_seq_length=256, max_tgt_length=64, memory_train_file='./preprocess/sim_result/sim_question_by_w2v_train_math23k_test1k_equNorm_top10.pkl', memory_valid_file='./preprocess/sim_result/sim_question_by_w2v_valid_math23k_test1k_equNorm_top10.pkl', min_len=None, mode='s2s', model_recover_path='./comment/math23k/REAL2_math23k_test1k_topn10_top3_cnn/model.epoch.bin', need_score_traces=True, new_pos_ids=False, new_segment_ids=False, ngram_size=3, no_cuda=False, not_predict_token=None, num_equ_size=3, num_qkv=0, num_train_epochs=50, num_workers=0, optim_recover_path=None, output_dir='./comment/math23k/REAL2_math23k_test1k_topn10_top3_cnn', pos_shift=True, pred_wo_memory_copy=False, relax_projection=False, repeat='', retrieve_model_name='cnn', retrieve_result_path='', retrieve_topn=10, s2s_add_segment=False, s2s_share_segment=False, s2s_special_token=False, save_every_epoch=False, seed=42, seg_emb=False, skipgram_prb=0.0, skipgram_size=1, split='test', start_lr_decay_epoch=25, subset=0, tokenized_input=False, topk=3, train_batch_size=1, train_question_sim_ids_path='./preprocess/raw_sim_dict/math23k_test1k_train_sim_id_top10.pkl', trunc_seg='a', used_bertAdam=False, used_lr_decay=True, valid_question_sim_ids_path='./preprocess/raw_sim_dict/math23k_test1k_valid_sim_id_top10.pkl', warmup_proportion=0.1, weight_decay=0.01)
[Wed, 03 Nov 2021 03:22:05] INFO [main: run_seq2seq_mwp.py, 71] device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
[Wed, 03 Nov 2021 03:22:07] INFO [from_pretrained: tokenization.py, 197] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at ..comment/tmp_folder/bert-cased-pretrained-cache/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00
[Wed, 03 Nov 2021 03:22:41] INFO [from_pretrained: modeling_mwp.py, 741] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at ..comment/tmp_folder/bert-cased-pretrained-cache/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f
[Wed, 03 Nov 2021 03:22:41] INFO [from_pretrained: modeling_mwp.py, 749] extracting archive file ..comment/tmp_folder/bert-cased-pretrained-cache/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir /tmp/tmpi0tkgwja
[Wed, 03 Nov 2021 03:22:59] INFO [from_pretrained: modeling_mwp.py, 843] config.type_vocab_size != state_dict[bert.embeddings.token_type_embeddings.weight] (4 != 2)
[Wed, 03 Nov 2021 03:22:59] INFO [from_pretrained: modeling_mwp.py, 1041] Weights of BertForPreTrainingLossMask not initialized from pretrained model: ['RetModel.embed.weight', 'RetModel.convs.0.weight', 'RetModel.convs.0.bias', 'RetModel.convs.1.weight', 'RetModel.convs.1.bias', 'RetModel.convs.2.weight', 'RetModel.convs.2.bias', 'RetModel.fc1.weight', 'RetModel.fc1.bias', 'bert.embeddings.num_equ_embeddings.weight', 'copyNet.bias', 'copyNet.decoder.weight', 'copyNet.p_gen_linear.weight', 'cls2.predictions.bias', 'cls2.predictions.transform.dense.weight', 'cls2.predictions.transform.dense.bias', 'cls2.predictions.transform.LayerNorm.weight', 'cls2.predictions.transform.LayerNorm.bias', 'cls2.predictions.decoder.weight', 'cls2.seq_relationship.weight', 'cls2.seq_relationship.bias', 'crit_mask_lm_smoothed.one_hot']
[Wed, 03 Nov 2021 03:23:02] INFO [main: run_seq2seq_mwp.py, 261] ***** CUDA.empty_cache() *****
[Wed, 03 Nov 2021 03:23:02] INFO [main: run_seq2seq_mwp.py, 265] ***** Running training *****
[Wed, 03 Nov 2021 03:23:02] INFO [main: run_seq2seq_mwp.py, 266]   Batch size = 1
[Wed, 03 Nov 2021 03:23:02] INFO [main: run_seq2seq_mwp.py, 267]   Num steps = 1107800
[Wed, 03 Nov 2021 03:23:02] INFO [main: run_seq2seq_mwp.py, 290] iter_bar:Iter (loss=X.XXX):   0%|                              | 0/22156 [00:00<?, ?it/s]
[Wed, 03 Nov 2021 03:23:02] INFO [main: run_seq2seq_mwp.py, 291] len of iter_bar:22156
[Wed, 03 Nov 2021 03:23:02] WARNING [forward: modeling_mwp.py, 1526] masked_lm_loss: 9.7393, copy_loss: 9.1796
