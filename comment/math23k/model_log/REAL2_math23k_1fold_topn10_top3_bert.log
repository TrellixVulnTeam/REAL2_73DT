[Tue, 02 Nov 2021 02:54:50] INFO [main: run_main_batch.py, 39] pid:39602, epoch:50
[Tue, 02 Nov 2021 02:54:50] INFO [main: run_main_batch.py, 40] args:Namespace(Fold=1, add_analogy_embedding=True, add_copynet=True, add_memory_module=True, add_num_equ_ids=True, add_tokens_a_padding=True, always_truncate_tail=True, amp=False, attention_probs_dropout_prob=0.1, batch_size=20, beam_size=1, bert_model='bert-base-chinese', config_path=None, copynet_name='copynet2', dataset='math23k', do_eval=False, do_l2r_training=False, do_lower_case=False, do_train=True, easy_to_hard=True, eval_batch_size=64, ffn_type=0, finetune_decay=False, forbid_duplicate_ngrams=False, forbid_ignore_word='.', fp16=False, fp32_embedding=False, from_scratch=False, gradient_accumulation_steps=1, has_sentence_oracle=False, hidden_dropout_prob=0.1, ids_questions_path='../preprocess/raw_sim_dict/raw_math23k_data_dict.pkl', is_debug=False, is_delete_early_model=False, is_equ_norm=True, is_single_char=False, is_train=False, label_smoothing=0.1, learning_rate=2e-05, length_penalty=0, local_rank=-1, log_dir='../comment/math23k/model_log', loss_scale=0, mask_prob=0.15, mask_prob_eos=0, mask_source_words=False, mask_whole_word=True, max_analogy_len=512, max_len_a=192, max_len_b=64, max_position_embeddings=512, max_pred=64, max_seq_length=256, max_tgt_length=64, memory_train_file='../preprocess/sim_result/sim_question_by_w2v_train_math23k_equNorm_1fold_top5.pkl', memory_valid_file='../preprocess/sim_result/sim_question_by_w2v_valid_math23k_equNorm_1fold_top5.pkl', min_len=None, mode='s2s', model_recover_path='../comment/math23k/REAL2_math23k_1fold_topn10_top3_bert/model.epoch.bin', need_score_traces=True, new_pos_ids=False, new_segment_ids=False, ngram_size=3, no_cuda=False, not_predict_token=None, num_equ_size=3, num_qkv=0, num_train_epochs=50, num_workers=0, optim_recover_path=None, output_dir='../comment/math23k/REAL2_math23k_1fold_topn10_top3_bert', pos_shift=True, pred_wo_memory_copy=False, relax_projection=False, repeat='', retrieve_model_name='bert', retrieve_result_path='../comment/math23k/retrieve_result/math23k_1fold_bert.csv', retrieve_topn=10, s2s_add_segment=False, s2s_share_segment=False, s2s_special_token=False, save_every_epoch=False, seed=42, seg_emb=False, skipgram_prb=0.0, skipgram_size=1, split='test', start_lr_decay_epoch=25, subset=0, tokenized_input=False, topk=3, train_batch_size=1, train_question_sim_ids_path='../preprocess/raw_sim_dict/math23k_train_sim_id_1fold.pkl', trunc_seg='a', used_bertAdam=False, used_lr_decay=True, valid_question_sim_ids_path='../preprocess/raw_sim_dict/math23k_valid_sim_id_1fold.pkl', warmup_proportion=0.1, weight_decay=0.01)
[Tue, 02 Nov 2021 02:54:53] INFO [from_pretrained: tokenization.py, 197] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at ..comment/tmp_folder/bert-cased-pretrained-cache/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00
[Tue, 02 Nov 2021 02:54:53] INFO [main_generation: decoder_seq2seq_mwp.py, 168] ../comment/math23k/REAL2_math23k_1fold_topn10_top3_bert/model.50.bin
[Tue, 02 Nov 2021 02:54:57] INFO [from_pretrained: modeling_mwp.py, 774] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at ..comment/tmp_folder/bert-cased-pretrained-cache/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f
[Tue, 02 Nov 2021 02:54:57] INFO [from_pretrained: modeling_mwp.py, 782] extracting archive file ..comment/tmp_folder/bert-cased-pretrained-cache/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir /tmp/tmp0d5f8vof
[Tue, 02 Nov 2021 02:55:15] INFO [from_pretrained: modeling_mwp.py, 1077] Weights from pretrained model not used in BertForSeq2SeqDecoder: ['cls2.predictions.bias', 'cls2.predictions.transform.dense.weight', 'cls2.predictions.transform.dense.bias', 'cls2.predictions.transform.LayerNorm.weight', 'cls2.predictions.transform.LayerNorm.bias', 'cls2.predictions.decoder.weight', 'cls2.seq_relationship.weight', 'cls2.seq_relationship.bias', 'crit_mask_lm_smoothed.one_hot']
[Tue, 02 Nov 2021 02:55:33] INFO [main_generation: decoder_seq2seq_mwp.py, 258] easy_nums:898, medium_nums:2287, upper_nums:906, hard_nums:540
[Tue, 02 Nov 2021 02:57:19] INFO [eval_5fold: eval.py, 115] acc_right: 0, question_total: 200, uneval: 200, correct score: 0.0000
[Tue, 02 Nov 2021 02:59:06] INFO [eval_5fold: eval.py, 115] acc_right: 0, question_total: 400, uneval: 400, correct score: 0.0000
[Tue, 02 Nov 2021 03:03:37] INFO [main: run_main_batch.py, 39] pid:41771, epoch:50
[Tue, 02 Nov 2021 03:03:37] INFO [main: run_main_batch.py, 40] args:Namespace(Fold=1, add_analogy_embedding=True, add_copynet=True, add_memory_module=True, add_num_equ_ids=True, add_tokens_a_padding=True, always_truncate_tail=True, amp=False, attention_probs_dropout_prob=0.1, batch_size=20, beam_size=1, bert_model='bert-base-chinese', config_path=None, copynet_name='copynet2', dataset='math23k', do_eval=False, do_l2r_training=False, do_lower_case=False, do_train=True, easy_to_hard=True, eval_batch_size=64, ffn_type=0, finetune_decay=False, forbid_duplicate_ngrams=False, forbid_ignore_word='.', fp16=False, fp32_embedding=False, from_scratch=False, gradient_accumulation_steps=1, has_sentence_oracle=False, hidden_dropout_prob=0.1, ids_questions_path='../preprocess/raw_sim_dict/raw_math23k_data_dict.pkl', is_debug=False, is_delete_early_model=False, is_equ_norm=True, is_single_char=False, is_train=False, label_smoothing=0.1, learning_rate=2e-05, length_penalty=0, local_rank=-1, log_dir='../comment/math23k/model_log', loss_scale=0, mask_prob=0.15, mask_prob_eos=0, mask_source_words=False, mask_whole_word=True, max_analogy_len=512, max_len_a=192, max_len_b=64, max_position_embeddings=512, max_pred=64, max_seq_length=256, max_tgt_length=64, memory_train_file='../preprocess/sim_result/sim_question_by_w2v_train_math23k_equNorm_1fold_top5.pkl', memory_valid_file='../preprocess/sim_result/sim_question_by_w2v_valid_math23k_equNorm_1fold_top5.pkl', min_len=None, mode='s2s', model_recover_path='../comment/math23k/REAL2_math23k_1fold_topn10_top3_bert/model.epoch.bin', need_score_traces=True, new_pos_ids=False, new_segment_ids=False, ngram_size=3, no_cuda=False, not_predict_token=None, num_equ_size=3, num_qkv=0, num_train_epochs=50, num_workers=0, optim_recover_path=None, output_dir='../comment/math23k/REAL2_math23k_1fold_topn10_top3_bert', pos_shift=True, pred_wo_memory_copy=False, relax_projection=False, repeat='', retrieve_model_name='bert', retrieve_result_path='../comment/math23k/retrieve_result/math23k_1fold_bert.csv', retrieve_topn=10, s2s_add_segment=False, s2s_share_segment=False, s2s_special_token=False, save_every_epoch=False, seed=42, seg_emb=False, skipgram_prb=0.0, skipgram_size=1, split='test', start_lr_decay_epoch=25, subset=0, tokenized_input=False, topk=3, train_batch_size=1, train_question_sim_ids_path='../preprocess/raw_sim_dict/math23k_train_sim_id_1fold.pkl', trunc_seg='a', used_bertAdam=False, used_lr_decay=True, valid_question_sim_ids_path='../preprocess/raw_sim_dict/math23k_valid_sim_id_1fold.pkl', warmup_proportion=0.1, weight_decay=0.01)
[Tue, 02 Nov 2021 03:03:38] INFO [from_pretrained: tokenization.py, 197] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at ..comment/tmp_folder/bert-cased-pretrained-cache/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00
[Tue, 02 Nov 2021 03:03:38] INFO [main_generation: decoder_seq2seq_mwp.py, 168] ../comment/math23k/REAL2_math23k_1fold_topn10_top3_bert/model.50.bin
[Tue, 02 Nov 2021 03:03:42] INFO [from_pretrained: modeling_mwp.py, 774] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at ..comment/tmp_folder/bert-cased-pretrained-cache/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f
[Tue, 02 Nov 2021 03:03:42] INFO [from_pretrained: modeling_mwp.py, 782] extracting archive file ..comment/tmp_folder/bert-cased-pretrained-cache/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir /tmp/tmpbga6cz0i
[Tue, 02 Nov 2021 03:04:01] INFO [from_pretrained: modeling_mwp.py, 1077] Weights from pretrained model not used in BertForSeq2SeqDecoder: ['cls2.predictions.bias', 'cls2.predictions.transform.dense.weight', 'cls2.predictions.transform.dense.bias', 'cls2.predictions.transform.LayerNorm.weight', 'cls2.predictions.transform.LayerNorm.bias', 'cls2.predictions.decoder.weight', 'cls2.seq_relationship.weight', 'cls2.seq_relationship.bias', 'crit_mask_lm_smoothed.one_hot']
[Tue, 02 Nov 2021 03:04:18] INFO [main_generation: decoder_seq2seq_mwp.py, 258] easy_nums:898, medium_nums:2287, upper_nums:906, hard_nums:540
[Tue, 02 Nov 2021 03:06:05] INFO [eval_5fold: eval.py, 115] acc_right: 0, question_total: 200, uneval: 200, correct score: 0.0000
[Tue, 02 Nov 2021 03:08:04] INFO [main: run_main_batch.py, 39] pid:43155, epoch:50
[Tue, 02 Nov 2021 03:08:04] INFO [main: run_main_batch.py, 40] args:Namespace(Fold=1, add_analogy_embedding=True, add_copynet=True, add_memory_module=True, add_num_equ_ids=True, add_tokens_a_padding=True, always_truncate_tail=True, amp=False, attention_probs_dropout_prob=0.1, batch_size=20, beam_size=1, bert_model='bert-base-chinese', config_path=None, copynet_name='copynet2', dataset='math23k', do_eval=False, do_l2r_training=False, do_lower_case=False, do_train=True, easy_to_hard=True, eval_batch_size=64, ffn_type=0, finetune_decay=False, forbid_duplicate_ngrams=False, forbid_ignore_word='.', fp16=False, fp32_embedding=False, from_scratch=False, gradient_accumulation_steps=1, has_sentence_oracle=False, hidden_dropout_prob=0.1, ids_questions_path='../preprocess/raw_sim_dict/raw_math23k_data_dict.pkl', is_debug=False, is_delete_early_model=False, is_equ_norm=True, is_single_char=False, is_train=False, label_smoothing=0.1, learning_rate=2e-05, length_penalty=0, local_rank=-1, log_dir='../comment/math23k/model_log', loss_scale=0, mask_prob=0.15, mask_prob_eos=0, mask_source_words=False, mask_whole_word=True, max_analogy_len=512, max_len_a=192, max_len_b=64, max_position_embeddings=512, max_pred=64, max_seq_length=256, max_tgt_length=64, memory_train_file='../preprocess/sim_result/sim_question_by_w2v_train_math23k_equNorm_1fold_top5.pkl', memory_valid_file='../preprocess/sim_result/sim_question_by_w2v_valid_math23k_equNorm_1fold_top5.pkl', min_len=None, mode='s2s', model_recover_path='../comment/math23k/REAL2_math23k_1fold_topn10_top3_bert/model.epoch.bin', need_score_traces=True, new_pos_ids=False, new_segment_ids=False, ngram_size=3, no_cuda=False, not_predict_token=None, num_equ_size=3, num_qkv=0, num_train_epochs=50, num_workers=0, optim_recover_path=None, output_dir='../comment/math23k/REAL2_math23k_1fold_topn10_top3_bert', pos_shift=True, pred_wo_memory_copy=False, relax_projection=False, repeat='', retrieve_model_name='bert', retrieve_result_path='../comment/math23k/retrieve_result/math23k_1fold_bert.csv', retrieve_topn=10, s2s_add_segment=False, s2s_share_segment=False, s2s_special_token=False, save_every_epoch=False, seed=42, seg_emb=False, skipgram_prb=0.0, skipgram_size=1, split='test', start_lr_decay_epoch=25, subset=0, tokenized_input=False, topk=3, train_batch_size=1, train_question_sim_ids_path='../preprocess/raw_sim_dict/math23k_train_sim_id_1fold.pkl', trunc_seg='a', used_bertAdam=False, used_lr_decay=True, valid_question_sim_ids_path='../preprocess/raw_sim_dict/math23k_valid_sim_id_1fold.pkl', warmup_proportion=0.1, weight_decay=0.01)
[Tue, 02 Nov 2021 03:08:05] INFO [from_pretrained: tokenization.py, 197] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at ..comment/tmp_folder/bert-cased-pretrained-cache/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00
[Tue, 02 Nov 2021 03:08:05] INFO [main_generation: decoder_seq2seq_mwp.py, 168] ../comment/math23k/REAL2_math23k_1fold_topn10_top3_bert/model.50.bin
[Tue, 02 Nov 2021 03:08:10] INFO [from_pretrained: modeling_mwp.py, 774] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at ..comment/tmp_folder/bert-cased-pretrained-cache/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f
[Tue, 02 Nov 2021 03:08:10] INFO [from_pretrained: modeling_mwp.py, 782] extracting archive file ..comment/tmp_folder/bert-cased-pretrained-cache/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir /tmp/tmp1rtlu0_f
[Tue, 02 Nov 2021 03:08:31] INFO [from_pretrained: modeling_mwp.py, 1077] Weights from pretrained model not used in BertForSeq2SeqDecoder: ['cls2.predictions.bias', 'cls2.predictions.transform.dense.weight', 'cls2.predictions.transform.dense.bias', 'cls2.predictions.transform.LayerNorm.weight', 'cls2.predictions.transform.LayerNorm.bias', 'cls2.predictions.decoder.weight', 'cls2.seq_relationship.weight', 'cls2.seq_relationship.bias', 'crit_mask_lm_smoothed.one_hot']
[Tue, 02 Nov 2021 03:09:42] INFO [main_generation: decoder_seq2seq_mwp.py, 258] easy_nums:898, medium_nums:2287, upper_nums:906, hard_nums:540
[Tue, 02 Nov 2021 03:16:11] INFO [main: run_main_batch.py, 39] pid:45159, epoch:50
[Tue, 02 Nov 2021 03:16:11] INFO [main: run_main_batch.py, 40] args:Namespace(Fold=1, add_analogy_embedding=True, add_copynet=True, add_memory_module=True, add_num_equ_ids=True, add_tokens_a_padding=True, always_truncate_tail=True, amp=False, attention_probs_dropout_prob=0.1, batch_size=20, beam_size=1, bert_model='bert-base-chinese', config_path=None, copynet_name='copynet2', dataset='math23k', do_eval=False, do_l2r_training=False, do_lower_case=False, do_train=True, easy_to_hard=True, eval_batch_size=64, ffn_type=0, finetune_decay=False, forbid_duplicate_ngrams=False, forbid_ignore_word='.', fp16=False, fp32_embedding=False, from_scratch=False, gradient_accumulation_steps=1, has_sentence_oracle=False, hidden_dropout_prob=0.1, ids_questions_path='../preprocess/raw_sim_dict/raw_math23k_data_dict.pkl', is_debug=False, is_delete_early_model=False, is_equ_norm=True, is_single_char=False, is_train=False, label_smoothing=0.1, learning_rate=2e-05, length_penalty=0, local_rank=-1, log_dir='../comment/math23k/model_log', loss_scale=0, mask_prob=0.15, mask_prob_eos=0, mask_source_words=False, mask_whole_word=True, max_analogy_len=512, max_len_a=192, max_len_b=64, max_position_embeddings=512, max_pred=64, max_seq_length=256, max_tgt_length=64, memory_train_file='../preprocess/sim_result/sim_question_by_w2v_train_math23k_equNorm_1fold_top5.pkl', memory_valid_file='../preprocess/sim_result/sim_question_by_w2v_valid_math23k_equNorm_1fold_top5.pkl', min_len=None, mode='s2s', model_recover_path='../comment/math23k/REAL2_math23k_1fold_topn10_top3_bert/model.epoch.bin', need_score_traces=True, new_pos_ids=False, new_segment_ids=False, ngram_size=3, no_cuda=False, not_predict_token=None, num_equ_size=3, num_qkv=0, num_train_epochs=50, num_workers=0, optim_recover_path=None, output_dir='../comment/math23k/REAL2_math23k_1fold_topn10_top3_bert', pos_shift=True, pred_wo_memory_copy=False, relax_projection=False, repeat='', retrieve_model_name='bert', retrieve_result_path='../comment/math23k/retrieve_result/math23k_1fold_bert.csv', retrieve_topn=10, s2s_add_segment=False, s2s_share_segment=False, s2s_special_token=False, save_every_epoch=False, seed=42, seg_emb=False, skipgram_prb=0.0, skipgram_size=1, split='test', start_lr_decay_epoch=25, subset=0, tokenized_input=False, topk=3, train_batch_size=1, train_question_sim_ids_path='../preprocess/raw_sim_dict/math23k_train_sim_id_1fold.pkl', trunc_seg='a', used_bertAdam=False, used_lr_decay=True, valid_question_sim_ids_path='../preprocess/raw_sim_dict/math23k_valid_sim_id_1fold.pkl', warmup_proportion=0.1, weight_decay=0.01)
[Tue, 02 Nov 2021 03:16:12] INFO [from_pretrained: tokenization.py, 197] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at ..comment/tmp_folder/bert-cased-pretrained-cache/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00
[Tue, 02 Nov 2021 03:16:12] INFO [main_generation: decoder_seq2seq_mwp.py, 168] ../comment/math23k/REAL2_math23k_1fold_topn10_top3_bert/model.50.bin
[Tue, 02 Nov 2021 03:16:17] INFO [from_pretrained: modeling_mwp.py, 774] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at ..comment/tmp_folder/bert-cased-pretrained-cache/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f
[Tue, 02 Nov 2021 03:16:17] INFO [from_pretrained: modeling_mwp.py, 782] extracting archive file ..comment/tmp_folder/bert-cased-pretrained-cache/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir /tmp/tmp34gy81u1
[Tue, 02 Nov 2021 03:16:36] INFO [from_pretrained: modeling_mwp.py, 1077] Weights from pretrained model not used in BertForSeq2SeqDecoder: ['cls2.predictions.bias', 'cls2.predictions.transform.dense.weight', 'cls2.predictions.transform.dense.bias', 'cls2.predictions.transform.LayerNorm.weight', 'cls2.predictions.transform.LayerNorm.bias', 'cls2.predictions.decoder.weight', 'cls2.seq_relationship.weight', 'cls2.seq_relationship.bias', 'crit_mask_lm_smoothed.one_hot']
[Tue, 02 Nov 2021 03:16:53] INFO [main_generation: decoder_seq2seq_mwp.py, 258] easy_nums:898, medium_nums:2287, upper_nums:906, hard_nums:540
[Tue, 02 Nov 2021 03:18:40] INFO [eval_5fold: eval.py, 107] acc_right: 181, question_total: 200, uneval: 0, correct score: 0.9050
[Tue, 02 Nov 2021 03:20:27] INFO [eval_5fold: eval.py, 107] acc_right: 364, question_total: 400, uneval: 0, correct score: 0.9100
[Tue, 02 Nov 2021 03:22:15] INFO [eval_5fold: eval.py, 107] acc_right: 548, question_total: 600, uneval: 0, correct score: 0.9133
[Tue, 02 Nov 2021 03:24:03] INFO [eval_5fold: eval.py, 107] acc_right: 730, question_total: 800, uneval: 1, correct score: 0.9125
[Tue, 02 Nov 2021 03:25:51] INFO [eval_5fold: eval.py, 107] acc_right: 905, question_total: 1000, uneval: 2, correct score: 0.9050
[Tue, 02 Nov 2021 03:27:39] INFO [eval_5fold: eval.py, 107] acc_right: 1083, question_total: 1200, uneval: 3, correct score: 0.9025
[Tue, 02 Nov 2021 03:29:26] INFO [eval_5fold: eval.py, 107] acc_right: 1263, question_total: 1400, uneval: 3, correct score: 0.9021
[Tue, 02 Nov 2021 03:31:14] INFO [eval_5fold: eval.py, 107] acc_right: 1446, question_total: 1600, uneval: 3, correct score: 0.9038
[Tue, 02 Nov 2021 03:33:01] INFO [eval_5fold: eval.py, 107] acc_right: 1626, question_total: 1800, uneval: 4, correct score: 0.9033
[Tue, 02 Nov 2021 03:34:49] INFO [eval_5fold: eval.py, 107] acc_right: 1806, question_total: 2000, uneval: 5, correct score: 0.9030
[Tue, 02 Nov 2021 03:36:37] INFO [eval_5fold: eval.py, 107] acc_right: 1987, question_total: 2200, uneval: 5, correct score: 0.9032
[Tue, 02 Nov 2021 03:38:25] INFO [eval_5fold: eval.py, 107] acc_right: 2166, question_total: 2400, uneval: 5, correct score: 0.9025
[Tue, 02 Nov 2021 03:40:13] INFO [eval_5fold: eval.py, 107] acc_right: 2354, question_total: 2600, uneval: 5, correct score: 0.9054
[Tue, 02 Nov 2021 03:42:01] INFO [eval_5fold: eval.py, 107] acc_right: 2527, question_total: 2800, uneval: 5, correct score: 0.9025
[Tue, 02 Nov 2021 03:43:48] INFO [eval_5fold: eval.py, 107] acc_right: 2702, question_total: 3000, uneval: 5, correct score: 0.9007
[Tue, 02 Nov 2021 03:45:36] INFO [eval_5fold: eval.py, 107] acc_right: 2877, question_total: 3200, uneval: 5, correct score: 0.8991
[Tue, 02 Nov 2021 03:47:24] INFO [eval_5fold: eval.py, 107] acc_right: 3019, question_total: 3400, uneval: 7, correct score: 0.8879
[Tue, 02 Nov 2021 03:49:12] INFO [eval_5fold: eval.py, 107] acc_right: 3174, question_total: 3600, uneval: 8, correct score: 0.8817
[Tue, 02 Nov 2021 03:51:00] INFO [eval_5fold: eval.py, 107] acc_right: 3324, question_total: 3800, uneval: 8, correct score: 0.8747
[Tue, 02 Nov 2021 03:52:48] INFO [eval_5fold: eval.py, 107] acc_right: 3481, question_total: 4000, uneval: 10, correct score: 0.8702
[Tue, 02 Nov 2021 03:54:36] INFO [eval_5fold: eval.py, 107] acc_right: 3621, question_total: 4200, uneval: 11, correct score: 0.8621
[Tue, 02 Nov 2021 03:56:24] INFO [eval_5fold: eval.py, 107] acc_right: 3745, question_total: 4400, uneval: 13, correct score: 0.8511
[Tue, 02 Nov 2021 03:58:12] INFO [eval_5fold: eval.py, 107] acc_right: 3844, question_total: 4600, uneval: 19, correct score: 0.8357
[Tue, 02 Nov 2021 03:58:30] INFO [eval_5fold: eval.py, 107] acc_right: 3851, question_total: 4631, uneval: 22, correct score: 0.8316
[Tue, 02 Nov 2021 03:58:30] INFO [eval_5fold: eval.py, 107] acc_right: 814, question_total: 898, uneval: 2, correct score: 0.9065
[Tue, 02 Nov 2021 03:58:30] INFO [eval_5fold: eval.py, 107] acc_right: 2053, question_total: 2287, uneval: 3, correct score: 0.8977
[Tue, 02 Nov 2021 03:58:30] INFO [eval_5fold: eval.py, 107] acc_right: 688, question_total: 906, uneval: 5, correct score: 0.7594
[Tue, 02 Nov 2021 03:58:30] INFO [eval_5fold: eval.py, 107] acc_right: 296, question_total: 540, uneval: 12, correct score: 0.5481
